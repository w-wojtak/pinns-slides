<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Solving NFEs using PINNs</title>
		<link rel="stylesheet" href="css/styles.css">

		    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    		</script>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/simple.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section class="title-slide">
					<h1 class="title">Solving Neural Field Equations using Physics Informed Neural Networks</h1>
					<div class="author">
						<p style="font-size:110%; "><strong>Weronika Wojtak</strong><sup style="font-size:60%; ">1, 2, 3 ✉️</sup></p>
<!--						<p class="email"><a href="mailto:w.wojtak@dei.uminho.pt">w.wojtak@dei.uminho.pt</a></p>-->
					</div>
<!--					<div class="affiliation">-->
<!--						<p class="small-font">Research Centre Algoritmi, University of Minho, Guimarães, Portugal</p>-->
<!--						<p class="small-font">Research Centre of Mathematics, University of Minho, Guimarães, Portugal</p>-->
<!--						<p class="small-font">Centro de Computação Gráfica, Guimarães, Portugal</p>-->
<!--					</div>-->
					<div class="author">
						<p style="font-size:110%; "><strong>Estela Bicho</strong><sup style="font-size:60%; ">1</sup></p>
					</div>
<!--					<div class="affiliation">-->
<!--						<p class="small-font">Research Centre Algoritmi, University of Minho, Guimarães, Portugal</p>-->
<!--					</div>-->
					<div class="author">
						<p style="font-size:110%; "><strong>Wolfram Erlhagen</strong><sup  style="font-size:60%; ">2</sup></p>
					</div>
					<div class="affiliation">
						<p class="small-font"><sup style="font-size:90%; ">1</sup> Research Centre Algoritmi, University of Minho, Guimarães, Portugal</p>
						<p class="small-font"><sup style="font-size:90%; ">2</sup> Research Centre of Mathematics, University of Minho, Guimarães, Portugal</p>
						<p class="small-font"><sup style="font-size:90%; ">3</sup> Centro de Computação Gráfica, Guimarães, Portugal</p>
						<p class="email"><a href="mailto:w.wojtak@dei.uminho.pt"> ✉️ w.wojtak@dei.uminho.pt</a></p>
					</div>
				</section>



				<section>
					<h3 class="slide-title">Neural Field Equations</h3>
					<div class="slide-content">
								<ul>
									<li>Describe the <strong>large-scale spatio-temporal dynamics<br> of neuronal populations</strong> in the cortex.</li>
									<li>Typically formalized by integro-differential equations (IDEs):</li>
									<div class="equation">
										\[
										\frac{\partial u(x,t)}{\partial t} = -u(x,t) + \int_{\Omega} w(|x-y|)f(u(y,t)-\kappa) \, dy
										\]
									</div>
									<li>Applications in <strong>neuroscience</strong> and <strong>robotics</strong>.</li>
									<li>TODO: add images</li>
								</ul>
					</div>
				</section>


				<section>
					<h3 class="slide-title">Neural Field Equations</h3>
					  <div class="slide-content">
						  <p style="font-size:50%; margin-bottom: 2cm;"> The mathematical analysis: the existence and stability of <strong>localized activation patterns</strong>,<br>
							  believed to represent a basis for cortical information processing.</p>
						<video width="640" height="360" controls>
						  <source src="Amari_input_bump.mp4" type="video/mp4">
						  Your browser does not support the video tag.
						</video>
					  </div>
				</section>


				<section>
					<h3 class="slide-title">Neural Field Equations</h3>
					  <div class="slide-content">

						  <ul>
							  <li>In general, no closed-form expressions can be
							  found for these patterns<br> so the solutions must be <strong>approximated numerically.</strong></li>
							  <li>This requires significant computational effort,<br>
							  mostly due to the discretization of the <strong>spatial convolution term.</strong></li>
							  <li>Many approaches proposed, including <strong>collocation techniques, <br>
							  Galerkin schemes, low-rank methods, quadrature rules.</strong></li>
							  <li>One of the most efficient ways: use of <strong>Fast Fourier Transforms (FFTs).</strong></li>
							  <li>Motivated by <strong>the rise of scientific machine learning (SciML)</strong>,<br>
							  we look for a new approach to solving this kind of equation.</li>
						  </ul>


<!--						  <p style="font-size:55%; margin-bottom: 1cm;"> In general, no closed-form expressions can be-->
<!--							  found for these patterns<br> so the solutions must be <strong>approximated numerically.</strong></p>-->

<!--						  <p style="font-size:55%; margin-bottom: 1cm;"> This requires significant computational effort,<br>-->
<!--							  mostly due to the discretization of the <strong>spatial convolution term.</strong></p>-->

<!--						  <p style="font-size:55%; margin-bottom: 1cm;"> Many approaches proposed, including <strong>collocation techniques, <br>-->
<!--							  Galerkin schemes, low-rank methods, quadrature rules.</strong></p>-->

<!--						  <p style="font-size:55%; margin-bottom: 1cm;"> One of the most efficient ways: use of <strong>Fast Fourier Transforms (FFTs).</strong></p>-->

<!--						  <p style="font-size:55%; margin-bottom: 2cm;"> Motivated by <strong>the rise of scientific machine learning (SciML)</strong>,<br>-->
<!--							  we look for a new approach to solving this kind of equation.</p>-->

					  </div>
				</section>


				<section>
				  <div class="twocolumn">
					<div>
					  <h3>Traditional ML</h3>
								<ul>
									<li>Relies on <strong>statistical patterns</strong> in the data.</li>
									<li>Depends on <strong>large amounts of labelled data</strong>.</li>
									<li><strong>May lack interpretability</strong>, considered as a "black box" in some cases.</li>
<!--									<li>Many frameworks and libraries for <strong>easier and faster implementation</strong>.</li>-->
									<li>Applications: e.g. image recognition, natural language processing, recommendation engines.</li>
								</ul>
					</div>
					<div>
					  <h3>Scientific ML</h3>
								<ul>
									<li>Integrates <strong>physical principles</strong> into ML process.</li>
									<li>Can handle situations with <strong>limited or no data</strong>.</li>
									<li><strong>Interpretability</strong>: incorporates prior knowledge about the system's behaviour.</li>
<!--									<li>Often involves <strong>custom implementation</strong>.</li>-->
									<li>Applications: e.g. solving differential equations, inverse problems, simulating physical phenomena.</li>
								</ul>
					</div>
				  </div>
				</section>


								<section>
				  <div class="twocolumn">
					<div>
					  <h3>Traditional ML</h3>
<!--								<ul>-->
									<p style="font-size:50%;"><strong>Many frameworks and libraries</strong>.</p>
									<img src="figures/hf.png" alt="hugging face" width="90%">
<!--								</ul>-->
					</div>
					<div>
					  <h3>Scientific ML</h3>
<!--								<ul>-->
									<p style="font-size:50%;"><strong>Custom implementations</strong>, some frameworks exist.</p>
									<img src="figures/deepxde.png" alt="PINN Schematic" width="90%">
									<img src="figures/sciann.png" alt="PINN Schematic" width="90%">
<!--								</ul>-->
					</div>
				  </div>
				</section>


				<section>
					<h3 class="slide-title">Physics informed neural networks</h3>
					<div class="slide-content">
								<ul>
									<li>Designed to solve problems involving <strong>Partial Differential Equations (PDEs).</strong></li>
									<li>The idea: <strong>add differential equations into the loss function</strong> when training the network.</li>
<!--									<li>Consist of two parts: a surrogate network and a residual network.</li>-->
									<li>Leverages <strong>automatic differentiation</strong> (known as algorithmic differentiation or AutoDiff).</li>
									<li>Different kinds of problems: integer-order PDEs, fractional PDEs, stochastic PDEs and integrodifferential equations (IDEs). </li>
<!--									<li>Introduced in 2019:</li>-->
<!--									<img src="figures/paper.png" alt="PINN Schematic" width="70%">-->
								</ul>
					</div>
				</section>


				<section>
					<h3 class="slide-title">Physics informed neural networks</h3>
					<div class="slide-content">
						<p style="font-size:50%;">Introduced in 2019:</p>
							<img src="figures/paper.png" alt="PINN Schematic" width="60%">
						<img src="figures/citations.png" alt="PINN Schematic" width="50%">

					</div>
				</section>

				<section>
					<h3 class="slide-title">Physics informed neural networks</h3>
					<div class="slide-content">
<!--						Consist of two parts:-->
								<ul>
<!--									<li>Consist of two parts: a deep learning network called an approximator or-->
<!--										a surrogate network, and a residual network encoding the governing PDEs.</li>-->
									<li>The surrogate network is trained to provide an approximate solution
										at given collocation points.</li>
									<li>The residual network receives the output of the surrogate network and
										calculates a residual value (also called loss function).</li>
								</ul>
						TODO: add image
					</div>
				</section>


				<section>
					<h3 class="slide-title">Physics informed neural networks</h3>
					<div class="slide-content">
						<p style="font-size:55%;"> PINNs can solve differential equations expressed in the form:</p>
									<div class="equation">
										\[
										\mathcal{F}(u(z); \gamma) = f(z)  \quad z \; {\rm in} \; \Omega, \\
										\quad \\
										\mathcal{B}(u(z)) = g(z) \quad z \; {\rm in} \; \partial \Omega,
										\]
									</div>
						<p style="font-size:55%;"> defined on the domain \( \Omega \subset \mathbb{R} \) with the boundary
							\( \partial \Omega\), where:</p>
							<ul>
								<li>\(z=[x_1, \ldots, x_{d-1}; t]\): the space-time coordinate vector,</li>
								<li>\(u\): the unknown solution,</li>
								<li>\(\gamma\): the parameters of the governing equation,</li>
								<li>\(f\): the function identifying the data of the problem,</li>
								<li> \( \mathcal{F}\): the non-linear differential operator.</li>
								<li> \( \mathcal{B}\): arbitrary initial or boundary conditions,</li>
								<li> \(g\):  the boundary function.</li>
							</ul>
					</div>
				</section>





				<section>
					<h3 class="slide-title">Amari Equation</h3>
					<div class="slide-content">
						<div class="equation-container">
							<div class="equation">
								<p style="font-size:120%;">We solve the canonical Amari equation defined on a 1D finite domain \( \Omega \)</p>
								\[
								\begin{aligned}
								\frac{\partial u(x,t)}{\partial t} = -u(x,t) + \int_{\Omega} w(|x-y|)f(u(y,t)-\kappa) \, dy, & \quad (x, t) \; {\rm in} \; \Omega \times M, \\
								u(x, 0) = u_0, & \quad x \; {\rm in} \; \Omega,
								\end{aligned}
								\]
							</div>
							<p class="equation-label">(1)</p>
						</div>

						<div class="equation-container">
							<div class="equation">
								<p style="font-size:120%;">The initial condition \( u_0 \) is given by</p>
								\[
								u_0 = u(x,0) = A_{0}e^{ \left( - x^2 / \sigma_{0}^{2} \right)}
								\]
							</div>
							<p class="equation-label">(2)</p>
						</div>
<!--						<p>And the boundary conditions are periodic.</p>-->
<!--						<p>\(M = [0, T]\) defines the time interval for the activity integration.</p>-->

						<div class="equation-container">
							<div class="equation">
								<p style="font-size:120%;">The oscillatory connectivity function \( w \) is given by</p>
								<div>
									\[
									w_{osc}(x) = A_{osc}e^{-b \vert c x \vert} (b \sin \vert c x \vert + \cos (c x)).
									\]
								</div>
							</div>
							<p class="equation-label">(3)</p>
						</div>

					</div>
				</section>

				<section>
					<h3 class="slide-title">PINNs and IDEs</h3>
					<div class="slide-content">
						<p style="font-size:50%; margin-top: 1cm;">
						<ul>
							<li>For the state-of-the-art PINN methods, <strong>integral discretization</strong> is a key prerequisite <br>
								in order that IDEs can be transformed into ordinary differential equations (ODEs).</li>
							<li>Integral discretization inevitably introduces <strong>discretization and truncation errors</strong>.</li>
							<li>Possible solution: PINNs with auxiliary outputs (A-PINNs) which approximate <br> the integrals in the governing equation.</li>
							<li>The integral term in the Amari equation is a spatial convolution and as such can be<br> efficiently computed using <strong>FFTs</strong>.</li>
						</ul>
						</p>
					</div>
				</section>


				<section>
					<h3 class="slide-title">PINN for Amari Equation</h3>
					<div class="slide-content">
						<img src="figures/pinn.png" alt="PINN Schematic" width="90%">
						<p style="font-size:50%; margin-top: 1cm;">
						<ul>
							<li>Implemented in PyTorch.</li>
							<li>A fully connected feed-forward NN with two hidden layers and 40 neurons in each layer.</li>
							<li>tanh as the activation function because it satisfies the smoothness requirements for PINNs.</li>
							<li>L-BFGS optimizer with 0.01 learning rate.</li>
						</ul>
						</p>
					</div>
				</section>


				<section>
					<h3 class="slide-title">The training data set</h3>
					<div class="slide-content">
<!--						<p style="font-size:70%; margin-top: 1cm;"> The training data set consists of two parts:-->
						<p style="font-size:60%; margin-top: 1cm; margin-bottom: 1cm;">The inputs \((x, t)\) to the neural network are the coordinates of the training points:</p>
							<ul>
								<li>500 initial points \( (x^{ini}_i,0)\) uniformly sampled at \(t = 0 \) .</li>
								<li>10000 collocation points \( (x^{f}_t,t^{f}_t)\), uniformly sampled in the equation domain.</li>
							</ul>
<!--						</p>-->
					</div>
				</section>


				<section>
					<h3 class="slide-title">The loss function</h3>
					<div class="slide-content">
							<ul>
								<li>Learning takes place by <strong>minimizing a loss function</strong> which depends on the governing equation and the initial conditions:</li>
							</ul>
						<img src="figures/lossf.png" alt="PINN Schematic" width="90%">
							<p style="font-size:50%; margin-top: 1cm;">
							<ul>
								<li>Use of <strong>FFTs</strong>  provides the <strong>periodic boundary conditions</strong> in \( x\), so no the need to include
									them in the loss function.</li>
							</ul>
							</p>
					</div>
				</section>




<!--				<section>-->
<!--					<h3 class="slide-title">Results</h3>-->
<!--					<div class="slide-content">-->
<!--						<img src="figures/results.png" alt="Results" width="90%">-->
<!--						<p class="caption">Localized activity pattern or bump solution of the Amari equation (1).-->
<!--							The approximation by the PINN and the respective approximation error (the difference between-->
<!--							the PINN solution and the one obtained by the forward Euler method) are shown for the time-->
<!--							interval \(M=[0,1]\) (a, b) and for snapshots at times \(t=0\) and \(t=1\) (c, d).-->
<!--							The initial condition at time \(t=0\) is given by (2) with \(A_0=0.9\) and \( \sigma_0=0.07\).-->
<!--							The connectivity function is (3) with \(A_{osc}=10\), \(b=0.7\) and \(c=24\).-->
<!--							Threshold \(\kappa=0.4\).</p>-->
<!--					</div>-->
<!--				</section>-->

				<section>
					<h3 class="slide-title">Results</h3>
					<div class="slide-content">
						<div class="twocolumn_result">
							<div>
							  <img src="figures/results_a.png" alt="Results" width="120%">
							</div>
							<div>
								<p style="font-size:50%; margin-top: 2cm;">
									Left: Bump solution after 500 iterations.</p>
								<p style="font-size:50%; margin-top: 1cm;">
									The approximation error is the difference between the PINN solution and the one obtained by the forward Euler method.</p>
							</div>
						</div>
					</div>
				</section>


				<section>
					<h3 class="slide-title">Results</h3>
					<div class="slide-content">
						<div class="twocolumn_result">
							<div>
							  <img src="figures/results_b.png" alt="Results" width="120%">
							</div>
							<div>
								<p style="font-size:50%; margin-top: 2cm;">
									 The relative L2 error between the PINN solution and the approximation obtained by the forward Euler method is 0.37%.
<!--									<strong>(d)</strong> Snapshot at time \(t=1\).-->
<!--								<p style="font-size:50%; margin-top: 1cm;">-->
<!--									The initial condition at time \(t=0\) is given by (2) with \(A_0=0.9\) and \( \sigma_0=0.07\). <br>-->
<!--									The connectivity function is (3) with \(A_{osc}=10\), \(b=0.7\) and \(c=24\).<br>-->
<!--									Threshold \(\kappa=0.4\).</p>-->
							</div>
						</div>
					</div>
				</section>


				<section>
					<h3 class="slide-title">Future work</h3>
					<div class="slide-content">
						<ul>
							<li>Mitigate the restriction imposed by the tanh function (e.g., add a normalization layer).</li>
						  	<li>Adjust hyperparameters (e.g., number of layers, neurons, learning rate) for better accuracy.</li>
						  	<li><strong>NFEs with inputs (!)</strong>, leveraging PINNs' transfer learning.</li>
						  	<li><strong>Inverse problem</strong>: inferring NFE parameters from potentially noisy data.</li>
						  	<li><strong>NFEs in two or more spatial dimensions</strong> or on surfaces with complex geometries.</li>
						</ul>
					</div>
				</section>


				<section>
					<h3 class="slide-title">References</h3>
					<div class="slide-content">
						<ul>
							<li>Amari, S. I. (1977). <strong> Dynamics of pattern formation in lateral-inhibition type neural fields</strong>. Biological Cybernetics, 27(2), 77-87.</li>
							<li>Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2019). <strong> Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</strong>. Journal of Computational Physics, 378, 686-707.</li>
							<li>Cuomo, S., Di Cola, V. S., Giampaolo, F., Rozza, G., Raissi, M., & Piccialli, F. (2022). <strong> Scientific machine learning through physics–informed neural networks: Where we are and what’s next.</strong> Journal of Scientific Computing, 92(3), 88.</li>
							<li>Markidis, S. (2021). <strong> The old and the new: Can physics-informed deep-learning replace traditional linear solvers?</strong> Frontiers in Big Data, 4, 669097. </li>
						</ul>
					</div>
				</section>





			</div>
		</div>


		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				slideNumber: 'c/t',

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
